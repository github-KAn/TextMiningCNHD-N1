new_list=["a","a", "a","a","b"]
text_collection=TextCollection(out_tokens)
print("\n text \n")
for text in new_list:
    print(text+":"+str(text_collection.tf_idf(text,new_list)))

words=out_tokens
word_freq = {}
freq = {}
for word in words:
    if word not in freq:
        freq[word] = 0
    # Increment the frequency of the word for a sentence
    freq[word] += 1
for word in freq:
    if word not in word_freq:
        word_freq[word] = 0
    # Increment the frequency of the word overall
    word_freq[word] += 1

# Frequency of terms in the document
print("freq of term: ",word_freq)


# import required module
from sklearn.feature_extraction.text import TfidfVectorizer

# Gán giá trị cho tài liệu
d0 = 'The cat jumped'
d1 = 'The white tiger roared'
d2 = 'Bird flying in the sky'

# Kết hợp tài liệu vào một tập hợp
# string = [d0, d1, d2]
string= ["The cat jumped",
        "The white tiger roared",
        "Bird flying in the sky"]
# Tạo đối tượng TfidfVectorier
tfidf = TfidfVectorizer()
# get tf-df values
result = tfidf.fit_transform(string)

# Lấy giá trị idf
print('\nidf values:')
for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):
    print(ele1, ':', ele2)

# Lấy chỉ số index
print('\nWord indexes:')
print(tfidf.vocabulary_)

# display tf-idf values
print('\ntf-idf value:')
print(result)

# in matrix form
print('\ntf-idf values in matrix form:')
print(result.toarray())

#TF-IDF
# doc1="Ben Studies Computer Lab"
# doc2="Steve teaches Brown University"
# doc3="Data scientists work large datasets"
# # doc4=[doc1,doc2,doc3]
# doc4={"doc1":doc1,"doc2":doc2,"doc3":doc3}
# doc4_str=doc1+doc2+doc3
# doc4_token=word_tokenize(doc4_str.lower())
# doc4_token=[word for word in doc4_token if word.isalpha() and word not in stopwords.words('english')]
# for term in doc4_token:
#     print(f"{term}: {inverseDocumentFrequency(term,doc4)}")

tfidf = TfidfVectorizer()

# get tf-df values
result = tfidf.fit_transform(string)

# get idf values
print('\nidf values:')
for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):
    print(ele1, ':', ele2)

# get indexing
print('\nWord indexes:')
print(tfidf.vocabulary_)

# display tf-idf values
print('\ntf-idf value:')
print(result)

# in matrix form
print('\ntf-idf values in matrix form:')
print(result.toarray())

with open('Processed Results/bow_text_rep.txt', 'w', encoding='utf-8') as file:
    file.write(str(word_counts))


def termFrequency(term, doc):
    """
    Input: term: Term in the Document, doc: Document
    Return: Normalized tf: Number of times term occurs
      in document/Total number of terms in the document
    """
    # Splitting the document into individual terms
    normalizeTermFreq = doc.lower().split()

    # Number of times the term occurs in the document
    term_in_document = normalizeTermFreq.count(term.lower())

    # Total number of terms in the document
    len_of_document = float(len(normalizeTermFreq))

    # Normalized Term Frequency
    normalized_tf = term_in_document / len_of_document

    return normalized_tf


def inverseDocumentFrequency(term, allDocs):
    num_docs_with_given_term = 0

    """
    Input: term: Term in the Document,
        allDocs: List of all documents
    Return: Inverse Document Frequency (idf) for term
            = Logarithm ((Total Number of Documents) /
            (Number of documents containing the term))
    """
    # Iterate through all the documents
    for doc in allDocs:

        """
        Putting a check if a term appears in a document.
        If term is present in the document, then
        increment "num_docs_with_given_term" variable
        """
        if term.lower() in allDocs[doc].lower().split():
            num_docs_with_given_term += 1

    if num_docs_with_given_term > 0:
        # Total number of documents
        total_num_docs = len(allDocs)

        # Calculating the IDF
        idf_val = log(float(total_num_docs) / num_docs_with_given_term)
        return idf_val
    else:
        return 0

data = {
  "calories": [420, 380, 390]
}

#load data into a DataFrame object:
df = pd.DataFrame(data)

print(df)



#load data đã được tiền xử lý và lưu vào file pickle
X_data = pickle.load(open('data/X_data_short.pkl', 'rb'))
y_data = pickle.load(open('data/y_data_short.pkl', 'rb'))

# X_test = pickle.load(open('data/X_test_short.pkl', 'rb'))
# y_test = pickle.load(open('data/y_test_short.pkl', 'rb'))



from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

#
# # create a count vectorizer object
# count_vect = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}')
# count_vect.fit(X_data)
#
# # transform the training and validation data using count vectorizer object
# X_data_count = count_vect.transform(X_data)
# X_test_count = count_vect.transform(X_test)
#
# # word level - we choose max number of words equal to 30000 except all words (100k+ words)
# tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)
# tfidf_vect.fit(X_data) # learn vocabulary and idf from training set
# X_data_tfidf =  tfidf_vect.transform(X_data)
# # assume that we don't have test set before
# X_test_tfidf =  tfidf_vect.transform(X_test)
#
# from sklearn.decomposition import TruncatedSVD
#
# svd = TruncatedSVD(n_components=300, random_state=42)
# svd.fit(X_data_tfidf)
#
#
# X_data_tfidf_svd = svd.transform(X_data_tfidf)
# X_test_tfidf_svd = svd.transform(X_test_tfidf)
# print(tfidf_vect.get_feature_names_out())
# print(X_data_tfidf_svd)
#
# data={}
# ft_names=X_data_tfidf_svd
# for i in range(len(ft_names)):
#     data[X_data[i]]=X_data_tfidf_svd[i]
# df=pd.DataFrame(data,index=ft_names)
# print(df.to_string())
# print(df.transpose())

vectorizer = TfidfVectorizer()
vectorizer.smooth_idf=False
print(X_data)
print(y_data)
data={}
id_matrix = vectorizer.fit_transform(X_data)
ft_names=vectorizer.get_feature_names_out()
print(ft_names)

a=id_matrix.toarray()
print(len(ft_names))
print(a, np.shape(a))
print(len(a), len(ft_names))
print(f"a[0] {a[0]} a[0][0] {a[0][0]}")
for i in range(len(a)):
    # data[f"doc{i}"]=a[i]
    data[y_data[i]]=a[i]
doc_name=[]
for i in range(len(y_data)):
    # doc_name.append(f"doc{i+1}")
    doc_name.append(y_data[i])
df=pd.DataFrame(data,index=ft_names)
print(df.to_string())
print((df.transpose()).to_string())
df.to_csv("train-tf-idf-news.csv",encoding="utf-8-sig")

doc_name=[]
for i in range(len(y_data)):
    doc_name.append(y_data[i])


#Bag of word
# Tạo mô hình bag-of-words
# word_counts = {}

#Tính tần suất xuất hiện từ
# for word in out_tokens:
#         if word in word_counts:
#             word_counts[word] += 1
#         else:
#             word_counts[word] = 1
# print("BOW:")

